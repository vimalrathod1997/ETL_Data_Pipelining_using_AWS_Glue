# ETL_Data_Pipelining_using_AWS_Glue
Create an ETL Pipeline in AWS to process large amounts of youtube data.

## Overview
The project has been carried out in AWS console. I am attaching a One note file that would serve as an document and workflow of this project.

## Files
**AWS_Data_Pipeline_Project_Notes.url** Is a OneNote file that contains all the details about the project and it's workflow.<br/>
**AWS S3 CLI copy commands.sh** contains the AWS CLI commands for copying data from system directory to AWS S3 bucket.<br/>
**AWS_Lambda_Function.py** contains the python script of AWS lambda function.<br/>
**AWS_Glue_Spark-job-convert-and-transform-csv-to-parquet.py** contains the python script generated by AWS glue for the spark job to process csv data.<br/>
**AWS_Glue_Spark-job-to-join-csv-and-json-data.py** contains the python script generated by AWS glue to join the clean tables and load data into Analytics S3 bucket.<br/>
**AWSLambdaBasicExecutionRole-a774d04d-ab42-4865-86b5-d515dd3638b4** Policy definition of AWS lambda role created while function creation.<br/>
**bigdata-on-youtube-process-read-write-on-s3-iam-poilcy** Policy definition of IAM role to read and write into specified S3 buckets.<br/>
**project1-big-data-on-youtube-glue-service-policy** Policy definition for IAM role to read and list specified S3 buckets to create Data catalog and subsequent databases.<br/>





